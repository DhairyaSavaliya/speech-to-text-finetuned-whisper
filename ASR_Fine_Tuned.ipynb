{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xqk75bFDoTRt",
        "outputId": "4b2dad1e-67af-4ef1-f5d4-7e384a1ac427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 Downloading from https://www.openslr.org/resources/12/train-clean-100.tar.gz...\n",
            "✅ File already downloaded: /content/librispeech/train-clean-100.tar.gz\n",
            "📦 File size: 6387309499 bytes\n",
            "🔄 Extracting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3176286600.py:58: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(libri_dir)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extraction successful!\n",
            "📁 Dataset ready at: /content/librispeech\n"
          ]
        }
      ],
      "source": [
        "# Fix download with resume capability and checks\n",
        "import os\n",
        "import tarfile\n",
        "import requests\n",
        "\n",
        "libri_dir = \"/content/librispeech\"\n",
        "os.makedirs(libri_dir, exist_ok=True)\n",
        "\n",
        "def download_file(url, destination):\n",
        "    \"\"\"Download file with resume capability\"\"\"\n",
        "    print(f\"📥 Downloading from {url}...\")\n",
        "\n",
        "    # Head request to get file size\n",
        "    response = requests.head(url)\n",
        "    file_size = int(response.headers.get('content-length', 0))\n",
        "\n",
        "    # Check if file already exists\n",
        "    if os.path.exists(destination):\n",
        "        current_size = os.path.getsize(destination)\n",
        "        if current_size == file_size:\n",
        "            print(f\"✅ File already downloaded: {destination}\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"🔄 Resuming download... ({current_size}/{file_size} bytes)\")\n",
        "\n",
        "    # Download with progress\n",
        "    headers = {}\n",
        "    if os.path.exists(destination):\n",
        "        current_size = os.path.getsize(destination)\n",
        "        headers = {'Range': f'bytes={current_size}-'}\n",
        "\n",
        "    response = requests.get(url, headers=headers, stream=True)\n",
        "    mode = 'ab' if headers else 'wb'\n",
        "\n",
        "    with open(destination, mode) as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "\n",
        "    print(f\"✅ Download completed: {destination}\")\n",
        "    return True\n",
        "\n",
        "# Download LibriSpeech\n",
        "url = \"https://www.openslr.org/resources/12/train-clean-100.tar.gz\"\n",
        "file_path = f\"{libri_dir}/train-clean-100.tar.gz\"\n",
        "\n",
        "# Download the file\n",
        "download_file(url, file_path)\n",
        "\n",
        "# Verify file size\n",
        "file_size = os.path.getsize(file_path)\n",
        "print(f\"📦 File size: {file_size} bytes\")\n",
        "\n",
        "# Extract\n",
        "print(\"🔄 Extracting...\")\n",
        "try:\n",
        "    with tarfile.open(file_path, 'r:gz') as tar:\n",
        "        tar.extractall(libri_dir)\n",
        "    print(\"✅ Extraction successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Extraction failed: {e}\")\n",
        "    print(\"🔄 Trying alternative extraction method...\")\n",
        "    !tar -xzf {file_path} -C {libri_dir}\n",
        "\n",
        "print(f\"📁 Dataset ready at: {libri_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's explore the dataset structure\n",
        "import os\n",
        "\n",
        "libri_path = \"/content/librispeech/LibriSpeech\"\n",
        "\n",
        "def explore_dataset(path):\n",
        "    print(\"📁 Dataset Structure:\")\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        level = root.replace(path, '').count(os.sep)\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for file in files[:5]:  # Show first 5 files\n",
        "            if file.endswith('.flac') or file.endswith('.txt'):\n",
        "                print(f\"{subindent}{file}\")\n",
        "        if len(files) > 5:\n",
        "            print(f\"{subindent}... and {len(files) - 5} more files\")\n",
        "\n",
        "explore_dataset(libri_path)\n",
        "\n",
        "# Check audio files\n",
        "print(\"\\n🎵 Checking audio files...\")\n",
        "audio_files = []\n",
        "for root, dirs, files in os.walk(libri_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.flac'):\n",
        "            audio_files.append(os.path.join(root, file))\n",
        "\n",
        "print(f\"Total audio files found: {len(audio_files)}\")\n",
        "if audio_files:\n",
        "    print(f\"Sample file: {audio_files[0]}\")"
      ],
      "metadata": {
        "id": "hKpFy0rLppsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required dependencies\n",
        "!pip install -q transformers datasets torch torchaudio librosa soundfile evaluate jiwer gradio\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import librosa\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor,\n",
        "    WhisperForConditionalGeneration, Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer\n",
        ")\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "import evaluate\n",
        "import gradio as gr\n",
        "import os\n",
        "\n",
        "print(\"✅ All dependencies installed!\")\n",
        "print(f\"🎯 CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Prepare the dataset\n",
        "def prepare_librispeech_dataset(data_path, num_samples=500):\n",
        "    \"\"\"Prepare LibriSpeech dataset for training\"\"\"\n",
        "    print(\"📁 Preparing LibriSpeech dataset...\")\n",
        "\n",
        "    audio_paths = []\n",
        "    texts = []\n",
        "\n",
        "    # Walk through all directories\n",
        "    for speaker_dir in os.listdir(data_path):\n",
        "        speaker_path = os.path.join(data_path, speaker_dir)\n",
        "        if not os.path.isdir(speaker_path):\n",
        "            continue\n",
        "\n",
        "        for chapter_dir in os.listdir(speaker_path):\n",
        "            chapter_path = os.path.join(speaker_path, chapter_dir)\n",
        "            if not os.path.isdir(chapter_path):\n",
        "                continue\n",
        "\n",
        "            # Look for .trans.txt file\n",
        "            trans_file = os.path.join(chapter_path, f\"{speaker_dir}-{chapter_dir}.trans.txt\")\n",
        "\n",
        "            if os.path.exists(trans_file):\n",
        "                with open(trans_file, 'r') as f:\n",
        "                    for line in f:\n",
        "                        if len(audio_paths) >= num_samples:\n",
        "                            break\n",
        "                        line = line.strip()\n",
        "                        if line:\n",
        "                            parts = line.split(' ', 1)\n",
        "                            if len(parts) == 2:\n",
        "                                audio_id, text = parts\n",
        "                                audio_file = os.path.join(chapter_path, f\"{audio_id}.flac\")\n",
        "                                if os.path.exists(audio_file):\n",
        "                                    audio_paths.append(audio_file)\n",
        "                                    texts.append(text)\n",
        "            if len(audio_paths) >= num_samples:\n",
        "                break\n",
        "        if len(audio_paths) >= num_samples:\n",
        "            break\n",
        "\n",
        "    print(f\"✅ Found {len(audio_paths)} audio-text pairs\")\n",
        "    return audio_paths, texts\n",
        "\n",
        "# Load data\n",
        "audio_paths, texts = prepare_librispeech_dataset(\"/content/librispeech/LibriSpeech/train-clean-100\", 500)\n",
        "\n",
        "# Create dataset\n",
        "dataset = Dataset.from_dict({\n",
        "    'audio_path': audio_paths,\n",
        "    'text': texts\n",
        "})\n",
        "\n",
        "print(\"Sample data:\")\n",
        "for i in range(min(3, len(dataset))):\n",
        "    print(f\"Text: {dataset[i]['text'][:80]}...\")\n",
        "    print(\"---\")\n",
        "\n",
        "# Load Whisper model and processor\n",
        "print(\"🧠 Loading Whisper model...\")\n",
        "model_name = \"openai/whisper-small\"\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\n",
        "tokenizer = WhisperTokenizer.from_pretrained(model_name, language=\"english\", task=\"transcribe\")\n",
        "processor = WhisperProcessor.from_pretrained(model_name, language=\"english\", task=\"transcribe\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Configure model\n",
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []\n",
        "model.generation_config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")\n",
        "\n",
        "print(\"✅ Model loaded!\")\n",
        "\n",
        "# Preprocess function\n",
        "def prepare_dataset(batch):\n",
        "    \"\"\"Preprocess audio and text\"\"\"\n",
        "    audio_path = batch[\"audio_path\"]\n",
        "    text = batch[\"text\"]\n",
        "\n",
        "    try:\n",
        "        # Load audio file\n",
        "        audio_array, sampling_rate = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "        # Compute input features\n",
        "        input_features = feature_extractor(\n",
        "            audio_array,\n",
        "            sampling_rate=16000\n",
        "        ).input_features[0]\n",
        "\n",
        "        # Encode target text\n",
        "        labels = tokenizer(text).input_ids\n",
        "\n",
        "        return {\n",
        "            \"input_features\": input_features,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Preprocess dataset\n",
        "print(\"🔄 Preprocessing dataset...\")\n",
        "processed_data = []\n",
        "for i in range(len(dataset)):\n",
        "    result = prepare_dataset(dataset[i])\n",
        "    if result is not None:\n",
        "        processed_data.append(result)\n",
        "\n",
        "print(f\"✅ Processed {len(processed_data)} samples\")\n",
        "\n",
        "# Create new dataset from processed data\n",
        "dataset = Dataset.from_list(processed_data)\n",
        "\n",
        "# Split dataset\n",
        "dataset_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = dataset_split[\"train\"]\n",
        "eval_dataset = dataset_split[\"test\"]\n",
        "\n",
        "print(f\"✅ Train samples: {len(train_dataset)}\")\n",
        "print(f\"✅ Eval samples: {len(eval_dataset)}\")\n",
        "\n",
        "# Verify sample\n",
        "print(\"\\nVerifying sample structure:\")\n",
        "sample = train_dataset[0]\n",
        "print(f\"input_features type: {type(sample['input_features'])}\")\n",
        "print(f\"input_features shape: {np.array(sample['input_features']).shape}\")\n",
        "print(f\"labels type: {type(sample['labels'])}\")\n",
        "print(f\"labels length: {len(sample['labels'])}\")\n",
        "\n",
        "# Custom Data Collator for Whisper\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # Split inputs and labels\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        # Pad input features\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # Pad labels\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # Replace padding with -100 to ignore loss\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # If bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "# Setup training\n",
        "print(\"⚙️ Setting up training...\")\n",
        "\n",
        "# Metrics\n",
        "wer_metric = evaluate.load(\"wer\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # Replace -100 with pad token\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Compute WER\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
        "\n",
        "# Training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"/content/whisper-librispeech-finetuned\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=50,\n",
        "    max_steps=200,\n",
        "    gradient_checkpointing=False,  # Disabled to avoid backward graph issues\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    logging_steps=25,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    load_best_model_at_end=False,\n",
        "    push_to_hub=False,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "print(\"✅ Training arguments set!\")\n",
        "\n",
        "# Create trainer\n",
        "print(\"🚀 Creating trainer...\")\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    processing_class=processor.feature_extractor,  # Updated parameter name\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"🎯 Starting training...\")\n",
        "training_result = trainer.train()\n",
        "\n",
        "print(\"✅ Fine-tuning completed!\")\n",
        "print(f\"Final training loss: {training_result.training_loss:.4f}\")\n",
        "\n",
        "# Save model\n",
        "print(\"💾 Saving model...\")\n",
        "trainer.save_model(\"/content/whisper-librispeech-finetuned\")\n",
        "processor.save_pretrained(\"/content/whisper-librispeech-finetuned\")\n",
        "\n",
        "# Test model\n",
        "print(\"🧪 Testing model...\")\n",
        "fine_tuned_model = WhisperForConditionalGeneration.from_pretrained(\"/content/whisper-librispeech-finetuned\")\n",
        "fine_tuned_processor = WhisperProcessor.from_pretrained(\"/content/whisper-librispeech-finetuned\")\n",
        "\n",
        "# Move model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    fine_tuned_model = fine_tuned_model.to(\"cuda\")\n",
        "\n",
        "print(\"Model loaded and ready!\")\n",
        "\n",
        "# Gradio app\n",
        "def transcribe_audio(audio_file):\n",
        "    if audio_file is None:\n",
        "        return \"Please upload or record audio\"\n",
        "\n",
        "    try:\n",
        "        audio_array, sampling_rate = librosa.load(audio_file, sr=16000)\n",
        "        inputs = fine_tuned_processor(\n",
        "            audio_array,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Move to GPU if available\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = inputs.to(\"cuda\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predicted_ids = fine_tuned_model.generate(inputs.input_features)\n",
        "\n",
        "        transcription = fine_tuned_processor.batch_decode(\n",
        "            predicted_ids,\n",
        "            skip_special_tokens=True\n",
        "        )[0]\n",
        "        return transcription\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "print(\"🎨 Launching Gradio app...\")\n",
        "iface = gr.Interface(\n",
        "    fn=transcribe_audio,\n",
        "    inputs=gr.Audio(sources=[\"upload\", \"microphone\"], type=\"filepath\"),\n",
        "    outputs=gr.Textbox(label=\"Transcription\"),\n",
        "    title=\"🎤 Fine-tuned Whisper Speech Recognition\",\n",
        "    description=\"Upload audio or record to get transcription from fine-tuned Whisper model\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lbODeT8s9ytE",
        "outputId": "95ad961e-ca68-4c7d-bd35-e948636a0e1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All dependencies installed!\n",
            "🎯 CUDA available: True\n",
            "📁 Preparing LibriSpeech dataset...\n",
            "✅ Found 500 audio-text pairs\n",
            "Sample data:\n",
            "Text: THE DIAMOND WEDDING BY EDMUND CLARENCE STEDMAN O LOVE LOVE LOVE WHAT TIMES WERE ...\n",
            "---\n",
            "Text: YOU MARRIED PSYCHE UNDER THE ROSE WITH ONLY THE GRASS FOR BEDDING HEART TO HEART...\n",
            "---\n",
            "Text: SO HAVE WE READ IN CLASSIC OVID HOW HERO WATCHED FOR HER BELOVED IMPASSIONED YOU...\n",
            "---\n",
            "🧠 Loading Whisper model...\n",
            "✅ Model loaded!\n",
            "🔄 Preprocessing dataset...\n",
            "✅ Processed 500 samples\n",
            "✅ Train samples: 400\n",
            "✅ Eval samples: 100\n",
            "\n",
            "Verifying sample structure:\n",
            "input_features type: <class 'list'>\n",
            "input_features shape: (80, 3000)\n",
            "labels type: <class 'list'>\n",
            "labels length: 70\n",
            "⚙️ Setting up training...\n",
            "✅ Training arguments set!\n",
            "🚀 Creating trainer...\n",
            "🎯 Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 18:53, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.645200</td>\n",
              "      <td>0.442556</td>\n",
              "      <td>0.143227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.260800</td>\n",
              "      <td>0.319572</td>\n",
              "      <td>0.101102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.122900</td>\n",
              "      <td>0.264208</td>\n",
              "      <td>0.104990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.067600</td>\n",
              "      <td>0.244650</td>\n",
              "      <td>0.103694</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fine-tuning completed!\n",
            "Final training loss: 0.3511\n",
            "💾 Saving model...\n",
            "🧪 Testing model...\n",
            "Model loaded and ready!\n",
            "🎨 Launching Gradio app...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://84e46913734b5a1ae6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://84e46913734b5a1ae6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kvi0YGri-VI-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}