# ğŸ™ï¸ Fine-Tuned Whisper for Speech-to-Text (English)

> **Fine-tuning OpenAIâ€™s Whisper model for English Automatic Speech Recognition (ASR)** with deployment via Gradio.  
> Implemented on Google Colab (T4 GPU) with Hugging Face Transformers and PyTorch.

---

## ğŸ§  Overview

This project fine-tunes **Whisper**, OpenAIâ€™s state-of-the-art speech recognition model, on a **custom English dataset** to improve transcription accuracy on real-world speech samples (accents, noise, conversational audio).

It demonstrates:
- How to **fine-tune large pre-trained ASR models**
- How to **deploy** them using a lightweight **Gradio web interface**
- How to evaluate model performance using **Word Error Rate (WER)** and **real audio tests**

This project is part of my **undergraduate research direction in AI/ML**, strengthening my portfolio for **MS in Computer Science / Data Science** in the USA.

---

## âš™ï¸ Key Highlights

âœ… Fine-tuned Whisper base model on a custom English dataset  
âœ… End-to-end pipeline: data preprocessing â†’ training â†’ evaluation â†’ deployment  
âœ… Evaluation via **Word Error Rate (WER)** metric  
âœ… Lightweight **Gradio app** for real-time speech transcription  
âœ… Transparent documentation of training logs and architecture choices  

---

## ğŸ—ï¸ Project Architecture

data/
â”œâ”€â”€ train/
â”œâ”€â”€ test/
â””â”€â”€ custom_audio_samples/
notebooks/
â”œâ”€â”€ ASR_Fine_Tuned.ipynb # Fine-tuning workflow
â””â”€â”€ Speech_to_Text_Baseline.ipynb # Pretrained Whisper demo & deployment
assets/
â””â”€â”€ demo_screenshot.png
requirements.txt
README.md


---

## ğŸš€ Workflow

1. **Dataset Preparation**
   - Custom English speech dataset prepared via web-scraping and manual cleaning.  
   - Converted to `.wav` format with consistent sample rates.  
   - Transcripts formatted to Hugging Face Dataset format.

2. **Model Fine-Tuning**
   - Model: `openai/whisper-base`  
   - Framework: PyTorch + Hugging Face Transformers  
   - Optimizer: AdamW  
   - Evaluation metric: Word Error Rate (WER)

3. **Evaluation**
   - Baseline Whisper model tested on LibriSpeech subset  
   - Fine-tuned version tested on domain-specific dataset  
   - Accuracy improvement observed qualitatively and quantitatively

4. **Deployment**
   - Deployed via **Gradio** for interactive speech-to-text transcription  
   - Simple web UI accepts `.wav` input or microphone input and displays real-time text output

---

## ğŸ“Š Fine-Tuning Performance (Word Error Rate)

| Fine-Tuning Round | Training Epochs | Learning Rate | Validation WER (%) | Observation |
|-------------------|-----------------|----------------|--------------------|--------------|
| Baseline (no tuning) | â€” | â€” | **13.5%** | Pretrained Whisper performance on domain data |
| Round 1 | 3 | 5e-5 | **10.1%** | Model adapting to accent and tone variations |
| Round 2 | 5 | 3e-5 | **10.49%** | Stable convergence; reduced substitution errors |
| Round 3 | 7 | 2e-5 | **10.3%** | Optimal trade-off between accuracy and overfitting |

> âœ… **Improvement:** 13.5 â†’ 10.1 WER (~25% relative improvement)  
> Fine-tuning carried out on Colab T4 GPU using 1.2 hours of total runtime.

---

## ğŸ§ª Example Output

| Model | Sample Input | Predicted Text |
|--------|---------------|----------------|
| Whisper Base | â€œhello everyone welcomeâ€¦â€ | â€œhello everyone welcomeâ€¦â€ | 
| Fine-tuned | â€œmy name is beva and i study aiâ€ | â€œmy name is beva and i study aiâ€ | 

---

## ğŸ–¼ï¸ Deployment Preview

| Fine-tuned Whisper Demo |
|--------------------------|
| ![Deployment Screenshot](asset/Demo_Screenshot.png) |

*(Executed in Google Colab using Gradio â€” can be replicated easily.)*

---


ğŸ’¬ Future Work

>Quantitative benchmarking on Common Voice dataset

>Experimenting with Whisper Medium or Large for multilingual ASR

>Exporting model as a FastAPI endpoint or Hugging Face Space

ğŸ“„ Tech Stack

|Category |	Tools |
|---------|--------|
|Framework |	PyTorch, Hugging Face Transformers |
|Deployment |	Gradio |
|Audio Processing |	Librosa, SoundFile, Torchaudio |
|Evaluation |	JiWER, Evaluate |
|Environment |	Google Colab (T4 GPU) |

ğŸ§‘â€ğŸ’» Author

Dhairya Savaliya |
B.Tech Computer Science | AI/ML Research Enthusiast
Passionate about Deep Learning, Speech Recognition, Machine Learning and Computer Vision

ğŸŒ GitHub : https://github.com/DhairyaSavaliya

ğŸ“§ Email : dhairyasavaliya73@gmail.com
